{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ElasticNotebook extension is already loaded. To reload it, use:\n",
      "  %reload_ext ElasticNotebook\n"
     ]
    }
   ],
   "source": [
    "# Get the data here: https://www.kaggle.com/datasets/ajayrana/hymenoptera-data\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"../..\"))\n",
    "%load_ext ElasticNotebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.shell.user_ns {'__name__': '__main__', '__doc__': 'Automatically created module for IPython interactive environment', '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '_ih': ['', '# Get the data here: https://www.kaggle.com/datasets/ajayrana/hymenoptera-data\\nimport sys, os\\nsys.path.insert(0, os.path.abspath(\"../..\"))\\nget_ipython().run_line_magic(\\'load_ext\\', \\'ElasticNotebook\\')', \"get_ipython().run_cell_magic('RecordEvent', '', 'import torch\\\\nimport torch.nn as nn\\\\nimport torch.optim as optim\\\\nfrom torch.optim import lr_scheduler\\\\nimport torch.backends.cudnn as cudnn\\\\nimport numpy as np\\\\nimport torchvision\\\\nfrom torchvision import datasets, models, transforms\\\\nimport matplotlib.pyplot as plt\\\\nimport time\\\\nimport os\\\\nimport copy\\\\n\\\\ncudnn.benchmark = True\\\\nplt.ion()   # interactive mode\\\\n')\", '# Get the data here: https://www.kaggle.com/datasets/ajayrana/hymenoptera-data\\nimport sys, os\\nsys.path.insert(0, os.path.abspath(\"../..\"))\\nget_ipython().run_line_magic(\\'load_ext\\', \\'ElasticNotebook\\')', \"get_ipython().run_cell_magic('RecordEvent', '', 'import torch\\\\nimport torch.nn as nn\\\\nimport torch.optim as optim\\\\nfrom torch.optim import lr_scheduler\\\\nimport torch.backends.cudnn as cudnn\\\\nimport numpy as np\\\\nimport torchvision\\\\nfrom torchvision import datasets, models, transforms\\\\nimport matplotlib.pyplot as plt\\\\nimport time\\\\nimport os\\\\nimport copy\\\\n\\\\ncudnn.benchmark = True\\\\nplt.ion()   # interactive mode\\\\n')\"], '_oh': {2: <contextlib.ExitStack object at 0x103f9d3d0>}, '_dh': [PosixPath('/Users/marioaranda/Documents/GitHub/CS511Project/elastic/examples'), PosixPath('/Users/marioaranda/Documents/GitHub/CS511Project/elastic/examples')], 'In': ['', '# Get the data here: https://www.kaggle.com/datasets/ajayrana/hymenoptera-data\\nimport sys, os\\nsys.path.insert(0, os.path.abspath(\"../..\"))\\nget_ipython().run_line_magic(\\'load_ext\\', \\'ElasticNotebook\\')', \"get_ipython().run_cell_magic('RecordEvent', '', 'import torch\\\\nimport torch.nn as nn\\\\nimport torch.optim as optim\\\\nfrom torch.optim import lr_scheduler\\\\nimport torch.backends.cudnn as cudnn\\\\nimport numpy as np\\\\nimport torchvision\\\\nfrom torchvision import datasets, models, transforms\\\\nimport matplotlib.pyplot as plt\\\\nimport time\\\\nimport os\\\\nimport copy\\\\n\\\\ncudnn.benchmark = True\\\\nplt.ion()   # interactive mode\\\\n')\", '# Get the data here: https://www.kaggle.com/datasets/ajayrana/hymenoptera-data\\nimport sys, os\\nsys.path.insert(0, os.path.abspath(\"../..\"))\\nget_ipython().run_line_magic(\\'load_ext\\', \\'ElasticNotebook\\')', \"get_ipython().run_cell_magic('RecordEvent', '', 'import torch\\\\nimport torch.nn as nn\\\\nimport torch.optim as optim\\\\nfrom torch.optim import lr_scheduler\\\\nimport torch.backends.cudnn as cudnn\\\\nimport numpy as np\\\\nimport torchvision\\\\nfrom torchvision import datasets, models, transforms\\\\nimport matplotlib.pyplot as plt\\\\nimport time\\\\nimport os\\\\nimport copy\\\\n\\\\ncudnn.benchmark = True\\\\nplt.ion()   # interactive mode\\\\n')\"], 'Out': {2: <contextlib.ExitStack object at 0x103f9d3d0>}, 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x103f1eb50>>, 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x103f32490>, 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x103f32490>, '_': <contextlib.ExitStack object at 0x103f9d3d0>, '__': '', '___': '', '__vsc_ipynb_file__': '/Users/marioaranda/Documents/GitHub/CS511Project/elastic/examples/pytorch.ipynb', '_i': '# Get the data here: https://www.kaggle.com/datasets/ajayrana/hymenoptera-data\\nimport sys, os\\nsys.path.insert(0, os.path.abspath(\"../..\"))\\n%load_ext ElasticNotebook', '_ii': '%%RecordEvent\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nfrom torch.optim import lr_scheduler\\nimport torch.backends.cudnn as cudnn\\nimport numpy as np\\nimport torchvision\\nfrom torchvision import datasets, models, transforms\\nimport matplotlib.pyplot as plt\\nimport time\\nimport os\\nimport copy\\n\\ncudnn.benchmark = True\\nplt.ion()   # interactive mode', '_iii': '# Get the data here: https://www.kaggle.com/datasets/ajayrana/hymenoptera-data\\nimport sys, os\\nsys.path.insert(0, os.path.abspath(\"../..\"))\\n%load_ext ElasticNotebook', '_i1': '# Get the data here: https://www.kaggle.com/datasets/ajayrana/hymenoptera-data\\nimport sys, os\\nsys.path.insert(0, os.path.abspath(\"../..\"))\\n%load_ext ElasticNotebook', 'sys': <module 'sys' (built-in)>, 'os': <module 'os' from '/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/os.py'>, '_i2': '%%RecordEvent\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nfrom torch.optim import lr_scheduler\\nimport torch.backends.cudnn as cudnn\\nimport numpy as np\\nimport torchvision\\nfrom torchvision import datasets, models, transforms\\nimport matplotlib.pyplot as plt\\nimport time\\nimport os\\nimport copy\\n\\ncudnn.benchmark = True\\nplt.ion()   # interactive mode', 'torch': <module 'torch' from '/Users/marioaranda/Documents/GitHub/CS511Project/.env/lib/python3.9/site-packages/torch/__init__.py'>, 'nn': <module 'torch.nn' from '/Users/marioaranda/Documents/GitHub/CS511Project/.env/lib/python3.9/site-packages/torch/nn/__init__.py'>, 'optim': <module 'torch.optim' from '/Users/marioaranda/Documents/GitHub/CS511Project/.env/lib/python3.9/site-packages/torch/optim/__init__.py'>, 'lr_scheduler': <module 'torch.optim.lr_scheduler' from '/Users/marioaranda/Documents/GitHub/CS511Project/.env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py'>, 'cudnn': <module 'torch.backends.cudnn' from '/Users/marioaranda/Documents/GitHub/CS511Project/.env/lib/python3.9/site-packages/torch/backends/cudnn/__init__.py'>, 'np': <module 'numpy' from '/Users/marioaranda/Documents/GitHub/CS511Project/.env/lib/python3.9/site-packages/numpy/__init__.py'>, 'torchvision': <module 'torchvision' from '/Users/marioaranda/Documents/GitHub/CS511Project/.env/lib/python3.9/site-packages/torchvision/__init__.py'>, 'datasets': <module 'torchvision.datasets' from '/Users/marioaranda/Documents/GitHub/CS511Project/.env/lib/python3.9/site-packages/torchvision/datasets/__init__.py'>, 'models': <module 'torchvision.models' from '/Users/marioaranda/Documents/GitHub/CS511Project/.env/lib/python3.9/site-packages/torchvision/models/__init__.py'>, 'transforms': <module 'torchvision.transforms' from '/Users/marioaranda/Documents/GitHub/CS511Project/.env/lib/python3.9/site-packages/torchvision/transforms/__init__.py'>, 'plt': <module 'matplotlib.pyplot' from '/Users/marioaranda/Documents/GitHub/CS511Project/.env/lib/python3.9/site-packages/matplotlib/pyplot.py'>, 'time': <module 'time' (built-in)>, 'copy': <module 'copy' from '/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/copy.py'>, '_2': <contextlib.ExitStack object at 0x103f9d3d0>, '_i3': '# Get the data here: https://www.kaggle.com/datasets/ajayrana/hymenoptera-data\\nimport sys, os\\nsys.path.insert(0, os.path.abspath(\"../..\"))\\n%load_ext ElasticNotebook', '_i4': '%%RecordEvent\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nfrom torch.optim import lr_scheduler\\nimport torch.backends.cudnn as cudnn\\nimport numpy as np\\nimport torchvision\\nfrom torchvision import datasets, models, transforms\\nimport matplotlib.pyplot as plt\\nimport time\\nimport os\\nimport copy\\n\\ncudnn.benchmark = True\\nplt.ion()   # interactive mode'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x17a967e20>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "variables to migrate:\n",
      "transforms 72\n",
      "datasets 72\n",
      "nn 72\n",
      "np 72\n",
      "optim 72\n",
      "time 72\n",
      "plt 72\n",
      "cudnn 72\n",
      "copy 72\n",
      "lr_scheduler 72\n",
      "torch 72\n",
      "models 72\n",
      "torchvision 72\n",
      "---------------------------\n",
      "variables to recompute:\n",
      "[]\n",
      "---------------------------\n",
      "cells to recompute:\n",
      "[]\n",
      "Checkpoint saved to: enhanced_checkpoint.pickle\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot pickle 'CudnnModule' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/marioaranda/Documents/GitHub/CS511Project/elastic/examples/pytorch.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/marioaranda/Documents/GitHub/CS511Project/elastic/examples/pytorch.ipynb#Y100sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39;49mrun_cell_magic(\u001b[39m'\u001b[39;49m\u001b[39mRecordEvent\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mimport torch\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mimport torch.nn as nn\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mimport torch.optim as optim\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mfrom torch.optim import lr_scheduler\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mimport torch.backends.cudnn as cudnn\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mimport numpy as np\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mimport torchvision\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mfrom torchvision import datasets, models, transforms\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mimport matplotlib.pyplot as plt\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mimport time\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mimport os\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mimport copy\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mcudnn.benchmark = True\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mplt.ion()   # interactive mode\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/GitHub/CS511Project/.env/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2362\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2360\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2361\u001b[0m     args \u001b[39m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2362\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2363\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Documents/GitHub/CS511Project/ElasticNotebook.py:190\u001b[0m, in \u001b[0;36mElasticNotebook.RecordEvent\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_recordevent_time \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m infer_end \u001b[39m-\u001b[39m infer_start\n\u001b[1;32m    189\u001b[0m \u001b[39m# Perform checkpointing\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mperform_checkpoint(checkpoint_data, \u001b[39m'\u001b[39;49m\u001b[39menhanced_checkpoint.pickle\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/GitHub/CS511Project/ElasticNotebook.py:223\u001b[0m, in \u001b[0;36mElasticNotebook.perform_checkpoint\u001b[0;34m(self, checkpoint_data, filename)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39mPerform the actual checkpointing by calling the checkpoint function.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[39m# The checkpoint function will need to be modified to accept the checkpoint_data format.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[39m# The following call assumes such modification has been done:\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m checkpoint(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdependency_graph, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshell, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfingerprint_dict, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mselector, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mudfs,\n\u001b[1;32m    224\u001b[0m             filename, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprofile_dict, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_log_location, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnotebook_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer_name,\n\u001b[1;32m    225\u001b[0m             checkpoint_data\u001b[39m=\u001b[39;49mcheckpoint_data)\n",
      "File \u001b[0;32m~/Documents/GitHub/CS511Project/elastic/core/notebook/checkpoint.py:125\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(graph, shell, fingerprint_dict, selector, udfs, filename, profile_dict, write_log_location, notebook_name, optimizer_name, checkpoint_data)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(filename, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    122\u001b[0m         \u001b[39m# protocol: https://docs.python.org/3/library/pickle.html#data-stream-format\u001b[39;00m\n\u001b[1;32m    123\u001b[0m         pickle\u001b[39m.\u001b[39mdump(checkpoint_data, f, protocol\u001b[39m=\u001b[39mpickle\u001b[39m.\u001b[39mHIGHEST_PROTOCOL)\n\u001b[0;32m--> 125\u001b[0m migrate(graph, shell, vss_to_migrate, vss_to_recompute, ces_to_recompute, udfs, selector\u001b[39m.\u001b[39;49mrecomputation_ces, selector\u001b[39m.\u001b[39;49moverlapping_vss, filename)\n\u001b[1;32m    126\u001b[0m migrate_end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    128\u001b[0m \u001b[39mif\u001b[39;00m write_log_location:\n",
      "File \u001b[0;32m~/Documents/GitHub/CS511Project/elastic/core/io/migrate.py:81\u001b[0m, in \u001b[0;36mmigrate\u001b[0;34m(graph, shell, vss_to_migrate, vss_to_recompute, ces_to_recompute, udfs, recomputation_ces, overlapping_vss, filename)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39mfor\u001b[39;00m vs \u001b[39min\u001b[39;00m vs_list:\n\u001b[1;32m     80\u001b[0m     obj_list\u001b[39m.\u001b[39mappend(shell\u001b[39m.\u001b[39muser_ns[vs\u001b[39m.\u001b[39mname])\n\u001b[0;32m---> 81\u001b[0m dill\u001b[39m.\u001b[39;49mdump(obj_list, output_file)\n",
      "File \u001b[0;32m~/Documents/GitHub/CS511Project/.env/lib/python3.9/site-packages/dill/_dill.py:276\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, file, protocol, byref, fmode, recurse, **kwds)\u001b[0m\n\u001b[1;32m    274\u001b[0m _kwds \u001b[39m=\u001b[39m kwds\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    275\u001b[0m _kwds\u001b[39m.\u001b[39mupdate(\u001b[39mdict\u001b[39m(byref\u001b[39m=\u001b[39mbyref, fmode\u001b[39m=\u001b[39mfmode, recurse\u001b[39m=\u001b[39mrecurse))\n\u001b[0;32m--> 276\u001b[0m Pickler(file, protocol, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_kwds)\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[1;32m    277\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/CS511Project/.env/lib/python3.9/site-packages/dill/_dill.py:498\u001b[0m, in \u001b[0;36mPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(msg)\n\u001b[1;32m    497\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 498\u001b[0m     StockPickler\u001b[39m.\u001b[39;49mdump(\u001b[39mself\u001b[39;49m, obj)\n\u001b[1;32m    499\u001b[0m stack\u001b[39m.\u001b[39mclear()  \u001b[39m# clear record of 'recursion-sensitive' pickled objects\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/pickle.py:487\u001b[0m, in \u001b[0;36m_Pickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[1;32m    486\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframer\u001b[39m.\u001b[39mstart_framing()\n\u001b[0;32m--> 487\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave(obj)\n\u001b[1;32m    488\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(STOP)\n\u001b[1;32m    489\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframer\u001b[39m.\u001b[39mend_framing()\n",
      "File \u001b[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/pickle.py:931\u001b[0m, in \u001b[0;36m_Pickler.save_list\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    928\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(MARK \u001b[39m+\u001b[39m LIST)\n\u001b[1;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemoize(obj)\n\u001b[0;32m--> 931\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_appends(obj)\n",
      "File \u001b[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/pickle.py:958\u001b[0m, in \u001b[0;36m_Pickler._batch_appends\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    956\u001b[0m     write(APPENDS)\n\u001b[1;32m    957\u001b[0m \u001b[39melif\u001b[39;00m n:\n\u001b[0;32m--> 958\u001b[0m     save(tmp[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m    959\u001b[0m     write(APPEND)\n\u001b[1;32m    960\u001b[0m \u001b[39m# else tmp is empty, and we're done\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/pickle.py:578\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    576\u001b[0m reduce \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m__reduce_ex__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 578\u001b[0m     rv \u001b[39m=\u001b[39m reduce(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproto)\n\u001b[1;32m    579\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    580\u001b[0m     reduce \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m__reduce__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle 'CudnnModule' object"
     ]
    }
   ],
   "source": [
    "%%RecordEvent\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%RecordEvent\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = 'data/hymenoptera_data'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%RecordEvent\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab526115",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%RecordEvent\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cc536c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%RecordEvent\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01997c19",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%RecordEvent\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fc7f0e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%RecordEvent\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faffc37b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%RecordEvent\n",
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34356f9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%RecordEvent\n",
    "visualize_model(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5340b02",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%RecordEvent\n",
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24112960",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%RecordEvent\n",
    "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf79aad",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%RecordEvent\n",
    "visualize_model(model_conv)\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ce1a8a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%RecordEvent\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8a05ad",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%RecordEvent\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fed09d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%RecordEvent\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4c8011",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%RecordEvent\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ae5981",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%RecordEvent\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c84426",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%RecordEvent\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0734c843",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%RecordEvent\n",
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e7ddd5",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%RecordEvent\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00bdc49",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%RecordEvent\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d107f1a9",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%RecordEvent\n",
    "outputs = net(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6fa0a3",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%RecordEvent\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0de2846",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%RecordEvent\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61a7586",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%RecordEvent\n",
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273ad5aa",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%RecordEvent\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%Checkpoint checkpoints/pytorch.pickle\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
