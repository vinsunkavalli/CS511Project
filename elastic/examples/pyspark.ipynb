{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"../..\"))\n",
    "%load_ext ElasticNotebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.shell.user_ns {'__name__': '__main__', '__doc__': 'Automatically created module for IPython interactive environment', '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '_ih': ['', 'import sys, os\\nsys.path.insert(0, os.path.abspath(\"../..\"))\\nget_ipython().run_line_magic(\\'load_ext\\', \\'ElasticNotebook\\')', \"get_ipython().run_cell_magic('RecordEvent', '', 'import findspark\\\\nfindspark.init()\\\\n\\\\nimport pyspark\\\\nimport random\\\\n')\"], '_oh': {}, '_dh': [PosixPath('/Users/marioaranda/Documents/GitHub/CS511Project/elastic/examples'), PosixPath('/Users/marioaranda/Documents/GitHub/CS511Project/elastic/examples')], 'In': ['', 'import sys, os\\nsys.path.insert(0, os.path.abspath(\"../..\"))\\nget_ipython().run_line_magic(\\'load_ext\\', \\'ElasticNotebook\\')', \"get_ipython().run_cell_magic('RecordEvent', '', 'import findspark\\\\nfindspark.init()\\\\n\\\\nimport pyspark\\\\nimport random\\\\n')\"], 'Out': {}, 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x10548eb50>>, 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x1054a2460>, 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x1054a2460>, '_': '', '__': '', '___': '', '__vsc_ipynb_file__': '/Users/marioaranda/Documents/GitHub/CS511Project/elastic/examples/pyspark.ipynb', '_i': 'import sys, os\\nsys.path.insert(0, os.path.abspath(\"../..\"))\\n%load_ext ElasticNotebook', '_ii': '', '_iii': '', '_i1': 'import sys, os\\nsys.path.insert(0, os.path.abspath(\"../..\"))\\n%load_ext ElasticNotebook', 'sys': <module 'sys' (built-in)>, 'os': <module 'os' from '/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/os.py'>, '_i2': '%%RecordEvent\\nimport findspark\\nfindspark.init()\\n\\nimport pyspark\\nimport random'}\n",
      "---------------------------\n",
      "variables to migrate:\n",
      "---------------------------\n",
      "variables to recompute:\n",
      "pyspark 72\n",
      "random 72\n",
      "findspark 72\n",
      "['pyspark', 'random', 'findspark']\n",
      "---------------------------\n",
      "cells to recompute:\n",
      "0 0.0009522438049316406\n",
      "[1]\n",
      "Checkpoint saved to: enhanced_checkpoint.pickle\n"
     ]
    }
   ],
   "source": [
    "%%RecordEvent\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.shell.user_ns {'__name__': '__main__', '__doc__': 'Automatically created module for IPython interactive environment', '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '_ih': ['', 'import sys, os\\nsys.path.insert(0, os.path.abspath(\"../..\"))\\nget_ipython().run_line_magic(\\'load_ext\\', \\'ElasticNotebook\\')', \"get_ipython().run_cell_magic('RecordEvent', '', 'import findspark\\\\nfindspark.init()\\\\n\\\\nimport pyspark\\\\nimport random\\\\n')\", 'get_ipython().run_cell_magic(\\'RecordEvent\\', \\'\\', \\'sc = pyspark.SparkContext(appName=\"Pi\")\\\\nnum_samples = 100000000\\\\n\\')'], '_oh': {}, '_dh': [PosixPath('/Users/marioaranda/Documents/GitHub/CS511Project/elastic/examples'), PosixPath('/Users/marioaranda/Documents/GitHub/CS511Project/elastic/examples')], 'In': ['', 'import sys, os\\nsys.path.insert(0, os.path.abspath(\"../..\"))\\nget_ipython().run_line_magic(\\'load_ext\\', \\'ElasticNotebook\\')', \"get_ipython().run_cell_magic('RecordEvent', '', 'import findspark\\\\nfindspark.init()\\\\n\\\\nimport pyspark\\\\nimport random\\\\n')\", 'get_ipython().run_cell_magic(\\'RecordEvent\\', \\'\\', \\'sc = pyspark.SparkContext(appName=\"Pi\")\\\\nnum_samples = 100000000\\\\n\\')'], 'Out': {}, 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x10548eb50>>, 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x1054a2460>, 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x1054a2460>, '_': '', '__': '', '___': '', '__vsc_ipynb_file__': '/Users/marioaranda/Documents/GitHub/CS511Project/elastic/examples/pyspark.ipynb', '_i': '%%RecordEvent\\nimport findspark\\nfindspark.init()\\n\\nimport pyspark\\nimport random', '_ii': 'import sys, os\\nsys.path.insert(0, os.path.abspath(\"../..\"))\\n%load_ext ElasticNotebook', '_iii': '', '_i1': 'import sys, os\\nsys.path.insert(0, os.path.abspath(\"../..\"))\\n%load_ext ElasticNotebook', 'sys': <module 'sys' (built-in)>, 'os': <module 'os' from '/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/os.py'>, '_i2': '%%RecordEvent\\nimport findspark\\nfindspark.init()\\n\\nimport pyspark\\nimport random', 'findspark': <module 'findspark' from '/Users/marioaranda/Documents/GitHub/CS511Project/.env/lib/python3.9/site-packages/findspark.py'>, 'pyspark': <module 'pyspark' from '/Users/marioaranda/Documents/GitHub/CS511Project/.env/lib/python3.9/site-packages/pyspark/__init__.py'>, 'random': <module 'random' from '/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/random.py'>, '_i3': '%%RecordEvent\\nsc = pyspark.SparkContext(appName=\"Pi\")\\nnum_samples = 100000000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/26 00:54:18 WARN Utils: Your hostname, MacBook-Pro-53.local resolves to a loopback address: 127.0.0.1; using 10.0.0.6 instead (on interface en0)\n",
      "23/11/26 00:54:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/marioaranda/Documents/GitHub/CS511Project/.env/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/26 00:54:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "variables to migrate:\n",
      "---------------------------\n",
      "variables to recompute:\n",
      "pyspark 72\n",
      "random 72\n",
      "num_samples 28\n",
      "sc inf\n",
      "findspark 72\n",
      "['pyspark', 'random', 'num_samples', 'sc', 'findspark']\n",
      "---------------------------\n",
      "cells to recompute:\n",
      "1 1.8905401229858398\n",
      "0 0.0009522438049316406\n",
      "[1, 2]\n",
      "Checkpoint saved to: enhanced_checkpoint.pickle\n"
     ]
    }
   ],
   "source": [
    "%%RecordEvent\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 100000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.shell.user_ns {'__name__': '__main__', '__doc__': 'Automatically created module for IPython interactive environment', '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '_ih': ['', 'import sys, os\\nsys.path.insert(0, os.path.abspath(\"../..\"))\\nget_ipython().run_line_magic(\\'load_ext\\', \\'ElasticNotebook\\')', \"get_ipython().run_cell_magic('RecordEvent', '', 'import findspark\\\\nfindspark.init()\\\\n\\\\nimport pyspark\\\\nimport random\\\\n')\", 'get_ipython().run_cell_magic(\\'RecordEvent\\', \\'\\', \\'sc = pyspark.SparkContext(appName=\"Pi\")\\\\nnum_samples = 100000000\\\\n\\')', \"get_ipython().run_cell_magic('RecordEvent', '', 'def inside(p):\\\\n  x, y = random.random(), random.random()\\\\n  return x*x + y*y < 1\\\\n')\"], '_oh': {}, '_dh': [PosixPath('/Users/marioaranda/Documents/GitHub/CS511Project/elastic/examples'), PosixPath('/Users/marioaranda/Documents/GitHub/CS511Project/elastic/examples')], 'In': ['', 'import sys, os\\nsys.path.insert(0, os.path.abspath(\"../..\"))\\nget_ipython().run_line_magic(\\'load_ext\\', \\'ElasticNotebook\\')', \"get_ipython().run_cell_magic('RecordEvent', '', 'import findspark\\\\nfindspark.init()\\\\n\\\\nimport pyspark\\\\nimport random\\\\n')\", 'get_ipython().run_cell_magic(\\'RecordEvent\\', \\'\\', \\'sc = pyspark.SparkContext(appName=\"Pi\")\\\\nnum_samples = 100000000\\\\n\\')', \"get_ipython().run_cell_magic('RecordEvent', '', 'def inside(p):\\\\n  x, y = random.random(), random.random()\\\\n  return x*x + y*y < 1\\\\n')\"], 'Out': {}, 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x10548eb50>>, 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x1054a2460>, 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x1054a2460>, '_': '', '__': '', '___': '', '__vsc_ipynb_file__': '/Users/marioaranda/Documents/GitHub/CS511Project/elastic/examples/pyspark.ipynb', '_i': '%%RecordEvent\\nsc = pyspark.SparkContext(appName=\"Pi\")\\nnum_samples = 100000000', '_ii': '%%RecordEvent\\nimport findspark\\nfindspark.init()\\n\\nimport pyspark\\nimport random', '_iii': 'import sys, os\\nsys.path.insert(0, os.path.abspath(\"../..\"))\\n%load_ext ElasticNotebook', '_i1': 'import sys, os\\nsys.path.insert(0, os.path.abspath(\"../..\"))\\n%load_ext ElasticNotebook', 'sys': <module 'sys' (built-in)>, 'os': <module 'os' from '/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/os.py'>, '_i2': '%%RecordEvent\\nimport findspark\\nfindspark.init()\\n\\nimport pyspark\\nimport random', 'findspark': <module 'findspark' from '/Users/marioaranda/Documents/GitHub/CS511Project/.env/lib/python3.9/site-packages/findspark.py'>, 'pyspark': <module 'pyspark' from '/Users/marioaranda/Documents/GitHub/CS511Project/.env/lib/python3.9/site-packages/pyspark/__init__.py'>, 'random': <module 'random' from '/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/random.py'>, '_i3': '%%RecordEvent\\nsc = pyspark.SparkContext(appName=\"Pi\")\\nnum_samples = 100000000', 'sc': <SparkContext master=local[*] appName=Pi>, 'num_samples': 100000000, '_i4': '%%RecordEvent\\ndef inside(p):\\n  x, y = random.random(), random.random()\\n  return x*x + y*y < 1'}\n",
      "---------------------------\n",
      "variables to migrate:\n",
      "num_samples 28\n",
      "sc 48\n",
      "---------------------------\n",
      "variables to recompute:\n",
      "pyspark 72\n",
      "random 72\n",
      "inside 136\n",
      "findspark 72\n",
      "['pyspark', 'random', 'inside', 'findspark']\n",
      "---------------------------\n",
      "cells to recompute:\n",
      "2 0.0004169940948486328\n",
      "0 0.0009522438049316406\n",
      "[1, 3]\n",
      "Checkpoint saved to: enhanced_checkpoint.pickle\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/marioaranda/Documents/GitHub/CS511Project/elastic/examples/pyspark.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/marioaranda/Documents/GitHub/CS511Project/elastic/examples/pyspark.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39;49mrun_cell_magic(\u001b[39m'\u001b[39;49m\u001b[39mRecordEvent\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mdef inside(p):\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m  x, y = random.random(), random.random()\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m  return x*x + y*y < 1\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/GitHub/CS511Project/.env/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2362\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2360\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2361\u001b[0m     args \u001b[39m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2362\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2363\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Documents/GitHub/CS511Project/ElasticNotebook.py:191\u001b[0m, in \u001b[0;36mElasticNotebook.RecordEvent\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_recordevent_time \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m infer_end \u001b[39m-\u001b[39m infer_start\n\u001b[1;32m    190\u001b[0m \u001b[39m# Perform checkpointing\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mperform_checkpoint(checkpoint_data, \u001b[39m'\u001b[39;49m\u001b[39menhanced_checkpoint.pickle\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/GitHub/CS511Project/ElasticNotebook.py:224\u001b[0m, in \u001b[0;36mElasticNotebook.perform_checkpoint\u001b[0;34m(self, checkpoint_data, filename)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39mPerform the actual checkpointing by calling the checkpoint function.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[39m# The checkpoint function will need to be modified to accept the checkpoint_data format.\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[39m# The following call assumes such modification has been done:\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m checkpoint(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdependency_graph, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshell, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfingerprint_dict, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mselector, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mudfs,\n\u001b[1;32m    225\u001b[0m             filename, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprofile_dict, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_log_location, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnotebook_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer_name,\n\u001b[1;32m    226\u001b[0m             checkpoint_data\u001b[39m=\u001b[39;49mcheckpoint_data)\n",
      "File \u001b[0;32m~/Documents/GitHub/CS511Project/elastic/core/notebook/checkpoint.py:125\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(graph, shell, fingerprint_dict, selector, udfs, filename, profile_dict, write_log_location, notebook_name, optimizer_name, checkpoint_data)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(filename, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    122\u001b[0m         \u001b[39m# protocol: https://docs.python.org/3/library/pickle.html#data-stream-format\u001b[39;00m\n\u001b[1;32m    123\u001b[0m         pickle\u001b[39m.\u001b[39mdump(checkpoint_data, f, protocol\u001b[39m=\u001b[39mpickle\u001b[39m.\u001b[39mHIGHEST_PROTOCOL)\n\u001b[0;32m--> 125\u001b[0m migrate(graph, shell, vss_to_migrate, vss_to_recompute, ces_to_recompute, udfs, selector\u001b[39m.\u001b[39;49mrecomputation_ces, selector\u001b[39m.\u001b[39;49moverlapping_vss, filename)\n\u001b[1;32m    126\u001b[0m migrate_end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    128\u001b[0m \u001b[39mif\u001b[39;00m write_log_location:\n",
      "File \u001b[0;32m~/Documents/GitHub/CS511Project/elastic/core/io/migrate.py:81\u001b[0m, in \u001b[0;36mmigrate\u001b[0;34m(graph, shell, vss_to_migrate, vss_to_recompute, ces_to_recompute, udfs, recomputation_ces, overlapping_vss, filename)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39mfor\u001b[39;00m vs \u001b[39min\u001b[39;00m vs_list:\n\u001b[1;32m     80\u001b[0m     obj_list\u001b[39m.\u001b[39mappend(shell\u001b[39m.\u001b[39muser_ns[vs\u001b[39m.\u001b[39mname])\n\u001b[0;32m---> 81\u001b[0m dill\u001b[39m.\u001b[39;49mdump(obj_list, output_file)\n",
      "File \u001b[0;32m~/Documents/GitHub/CS511Project/.env/lib/python3.9/site-packages/dill/_dill.py:276\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, file, protocol, byref, fmode, recurse, **kwds)\u001b[0m\n\u001b[1;32m    274\u001b[0m _kwds \u001b[39m=\u001b[39m kwds\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    275\u001b[0m _kwds\u001b[39m.\u001b[39mupdate(\u001b[39mdict\u001b[39m(byref\u001b[39m=\u001b[39mbyref, fmode\u001b[39m=\u001b[39mfmode, recurse\u001b[39m=\u001b[39mrecurse))\n\u001b[0;32m--> 276\u001b[0m Pickler(file, protocol, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_kwds)\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[1;32m    277\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/CS511Project/.env/lib/python3.9/site-packages/dill/_dill.py:498\u001b[0m, in \u001b[0;36mPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(msg)\n\u001b[1;32m    497\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 498\u001b[0m     StockPickler\u001b[39m.\u001b[39;49mdump(\u001b[39mself\u001b[39;49m, obj)\n\u001b[1;32m    499\u001b[0m stack\u001b[39m.\u001b[39mclear()  \u001b[39m# clear record of 'recursion-sensitive' pickled objects\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/pickle.py:487\u001b[0m, in \u001b[0;36m_Pickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[1;32m    486\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframer\u001b[39m.\u001b[39mstart_framing()\n\u001b[0;32m--> 487\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave(obj)\n\u001b[1;32m    488\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(STOP)\n\u001b[1;32m    489\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframer\u001b[39m.\u001b[39mend_framing()\n",
      "File \u001b[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/pickle.py:931\u001b[0m, in \u001b[0;36m_Pickler.save_list\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    928\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(MARK \u001b[39m+\u001b[39m LIST)\n\u001b[1;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemoize(obj)\n\u001b[0;32m--> 931\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_appends(obj)\n",
      "File \u001b[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/pickle.py:958\u001b[0m, in \u001b[0;36m_Pickler._batch_appends\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    956\u001b[0m     write(APPENDS)\n\u001b[1;32m    957\u001b[0m \u001b[39melif\u001b[39;00m n:\n\u001b[0;32m--> 958\u001b[0m     save(tmp[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m    959\u001b[0m     write(APPEND)\n\u001b[1;32m    960\u001b[0m \u001b[39m# else tmp is empty, and we're done\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/pickle.py:578\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    576\u001b[0m reduce \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m__reduce_ex__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 578\u001b[0m     rv \u001b[39m=\u001b[39m reduce(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproto)\n\u001b[1;32m    579\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    580\u001b[0m     reduce \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m__reduce__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/GitHub/CS511Project/.env/lib/python3.9/site-packages/pyspark/context.py:361\u001b[0m, in \u001b[0;36mSparkContext.__getnewargs__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getnewargs__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    360\u001b[0m     \u001b[39m# This method is called when attempting to pickle SparkContext, which is always an error:\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    362\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt appears that you are attempting to reference SparkContext from a broadcast \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mvariable, action, or transformation. SparkContext can only be used on the driver, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnot in code that it run on workers. For more information, see SPARK-5063.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063."
     ]
    }
   ],
   "source": [
    "%%RecordEvent\n",
    "def inside(p):\n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%RecordEvent\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%Checkpoint checkpoints/pyspark.pickle\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
